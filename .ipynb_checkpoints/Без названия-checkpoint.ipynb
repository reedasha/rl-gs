{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "import torch\n",
    "torch.manual_seed(0) # set random seed\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matlab.engine\n",
    "import time\n",
    "import random\n",
    "from numpy.random import randn\n",
    "from numpy.random import rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        # print(x.size())\n",
    "        # print('forward')\n",
    "        x = self.fc1(x)\n",
    "        # print('forward1')\n",
    "        x = self.fc2(x)\n",
    "        # print('forward3')\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.tensor(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(OSNR, M, policy, optimizer, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "\n",
    "    # Connect to matlab\n",
    "    eng = matlab.engine.start_matlab()\n",
    "    eng.cd(r'/home/reedvl/Desktop/Nokia/Simulation/BlackBox/QAMBlackBox_Daria', nargout=0)\n",
    "    \n",
    "     # Create file with current timestamp\n",
    "    current_date_and_time = datetime.datetime.now()\n",
    "    current_date_and_time_string = str(current_date_and_time)\n",
    "    extension = \".txt\"\n",
    "    file_name =  current_date_and_time_string + extension\n",
    "    f = open(file_name, \"a\")\n",
    "    \n",
    "    rs = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state, init_mi = eng.init(M, OSNR, nargout=2)\n",
    "        print('i got the init')\n",
    "        mis = [0]*max_t\n",
    "        actions = [0]*max_t\n",
    "        best_mi = 0\n",
    "        broken_loop = False\n",
    "\n",
    "        tic = time.perf_counter()\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, mi, done = eng.mi(action, state, OSNR, init_mi, nargout = 4)\n",
    "            mis[t] = mi\n",
    "            actions[t] = action\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            print('action', action, 't', t, 'ep:', i_episode, 'mi', round(mi, 2), 'reward', round(reward, 2), 'cum', round(sum(rewards), 2))\n",
    "            if mi > best_mi:\n",
    "                best_mi = mi\n",
    "                \n",
    "            # if actions[t] == actions[t-1] and t > 5:\n",
    "            #     if actions[t] == actions[t-2]:\n",
    "            #         if actions[t] == actions[t-3]:\n",
    "            #             break\n",
    "            # Plot the new constellation\n",
    "            # np_state = np.array(state)\n",
    "            # x = np.zeros(len(np_state))\n",
    "            # y = np.zeros(len(np_state))\n",
    "            # for i in range(len(np_state)):\n",
    "            #     x[i] = np_state[i, 0]\n",
    "            #     y[i] = np_state[i, 1]\n",
    "            \n",
    "            # plt.plot(x, y, 'ro')\n",
    "            # plt.show()\n",
    "            \n",
    "            # print('mi', mi)\n",
    "            if done:\n",
    "                broken_loop = True\n",
    "                break\n",
    "        \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        \n",
    "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        rs.append(float(policy_loss))\n",
    "        \n",
    "        # Save all the data\n",
    "        f.write('state: ')\n",
    "        f.write(str(state))\n",
    "        f.write('rewards: ')\n",
    "        f.write(str(rewards))\n",
    "        f.write('\\n')\n",
    "        f.write('scores: ')\n",
    "        f.write(str(scores))\n",
    "        f.write('\\n')\n",
    "        f.write('policy loss: ')\n",
    "        f.write(str(policy_loss))\n",
    "        f.write('\\n')\n",
    "        f.write('best_mi: ')\n",
    "        f.write(str(best_mi))\n",
    "        f.write('\\n')\n",
    "        if broken_loop:\n",
    "            f.write('BROKE THE LOOP')\n",
    "            f.write('\\n')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        episodes = [i for i in range(i_episode)]\n",
    "        plt.plot(episodes, scores, '.-')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Score')\n",
    "        # plt.savefig('scores_2.png')\n",
    "        plt.show()\n",
    "        \n",
    "        toc = time.perf_counter()\n",
    "        print(f\"RL episode performed in {toc - tic:0.4f} seconds\")\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "    f.close()\n",
    "    print(scores)\n",
    "    \n",
    "    episodes = [i for i in range(n_episodes)]\n",
    "    plt.plot(episodes, rs, '.-')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Policy loss')\n",
    "    plt.savefig('rewards_1.png')\n",
    "    plt.show()\n",
    "    \n",
    "    model_name = current_date_and_time_string + '_model' + '.pth'\n",
    "    torch.save({\n",
    "        'episode': n_episodes,\n",
    "        'max_t': max_t,\n",
    "        'model_state_dict': policy.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, model_name)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i got the init\n",
      "action 32 t 0 ep: 1 mi 2.67 reward 0.0 cum 0.0\n",
      "action 25 t 1 ep: 1 mi 2.67 reward 0.01 cum 0.01\n",
      "action 2 t 2 ep: 1 mi 2.67 reward 0.0 cum 0.02\n",
      "action 13 t 3 ep: 1 mi 2.67 reward 0.0 cum 0.02\n",
      "action 46 t 4 ep: 1 mi 2.67 reward 0.01 cum 0.03\n",
      "action 42 t 5 ep: 1 mi 2.67 reward 0.0 cum 0.03\n",
      "action 10 t 6 ep: 1 mi 2.65 reward -0.01 cum 0.02\n",
      "action 40 t 7 ep: 1 mi 2.67 reward 0.0 cum 0.02\n",
      "action 61 t 8 ep: 1 mi 2.67 reward 0.0 cum 0.02\n",
      "action 55 t 9 ep: 1 mi 2.67 reward 0.0 cum 0.02\n",
      "action 14 t 10 ep: 1 mi 2.67 reward 0.01 cum 0.03\n",
      "action 8 t 11 ep: 1 mi 2.67 reward 0.01 cum 0.04\n",
      "action 1 t 12 ep: 1 mi 2.66 reward -0.0 cum 0.04\n",
      "action 20 t 13 ep: 1 mi 2.66 reward -0.01 cum 0.03\n",
      "action 11 t 14 ep: 1 mi 2.66 reward -0.01 cum 0.03\n",
      "action 38 t 15 ep: 1 mi 2.67 reward 0.0 cum 0.03\n",
      "action 13 t 16 ep: 1 mi 2.66 reward -0.0 cum 0.03\n",
      "action 14 t 17 ep: 1 mi 2.66 reward -0.0 cum 0.02\n",
      "action 63 t 18 ep: 1 mi 2.66 reward -0.0 cum 0.02\n",
      "action 56 t 19 ep: 1 mi 2.67 reward 0.0 cum 0.02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.02\n",
      "i got the init\n",
      "action 55 t 0 ep: 2 mi 2.67 reward 0.0 cum 0.0\n",
      "action 8 t 1 ep: 2 mi 2.67 reward 0.01 cum 0.01\n",
      "action 34 t 2 ep: 2 mi 2.67 reward 0.0 cum 0.01\n",
      "action 13 t 3 ep: 2 mi 2.67 reward 0.0 cum 0.01\n",
      "action 48 t 4 ep: 2 mi 2.66 reward -0.0 cum 0.01\n",
      "action 44 t 5 ep: 2 mi 2.67 reward 0.0 cum 0.01\n",
      "action 36 t 6 ep: 2 mi 2.66 reward -0.0 cum 0.01\n",
      "action 0 t 7 ep: 2 mi 2.66 reward -0.0 cum 0.0\n",
      "action 42 t 8 ep: 2 mi 2.65 reward -0.02 cum -0.02\n",
      "action 43 t 9 ep: 2 mi 2.65 reward -0.01 cum -0.03\n",
      "action 20 t 10 ep: 2 mi 2.65 reward -0.01 cum -0.04\n",
      "action 52 t 11 ep: 2 mi 2.64 reward -0.02 cum -0.06\n",
      "action 30 t 12 ep: 2 mi 2.65 reward -0.02 cum -0.08\n",
      "the MATLAB function has been cancelled\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ef2077516eea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOSNR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-f450f5435c67>\u001b[0m in \u001b[0;36mreinforce\u001b[0;34m(OSNR, M, policy, optimizer, n_episodes, max_t, gamma, print_every)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0msaved_log_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSNR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnargout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mmis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "OSNR = 6\n",
    "M = 16\n",
    "n_episodes=10;\n",
    "max_t=20;\n",
    "gamma=1.0;\n",
    "print_every=1;\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy = Policy(s_size=2*M,h_size=5*M,a_size=4*M).to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "scores = reinforce(OSNR, M, policy, optimizer, n_episodes, max_t, gamma, print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
